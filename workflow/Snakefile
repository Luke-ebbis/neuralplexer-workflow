## NeuralPlexer workflow for investigating protein-ligand binding.
## 
## A workflow to predict the interaction between proteins and Ligands using
## NeuralPlexer by Qiao et al. Right now, the usage of PDB files with SIF files
## as ligands is supported.
## 
## Workflow made by Sibbe Bakker

import json
from pathlib import Path
import os


DATA_DIR = "data/"
OUTDIR = "results/data/"


def parse_json(file_path: str) -> Dict:
    with open(file_path, 'r') as file:
        data = json.load(file)
        return data


def get_structures(path, outdir, type="prot_all.pdb", name_only=True, folder="analysis"):
  json_files = [f"{DATA_DIR}/{name}"
   for name in os.listdir(path) if name.endswith("json")]
  structures = []
  for json in json_files:
    jobname = Path(json).stem
    data = parse_json(json)
    for job in data:
      name = job["name"]
      if name_only:
        file=f"{name}{type}"
      else:
        file = f"results/{folder}/{jobname}/{name}"
      structures.append(file)
  return set(structures)


STRUCTURES = get_structures(DATA_DIR, OUTDIR, name_only=False)


METRICS = [
  'sasa', 
  'plddt',
  "ramachandran",
  "clashes"
]

print(STRUCTURES)
## output:
##    A directory with SIF files, holding the different possible locations of
##    the predicted ligands, and the pdb file with the receptor structure.
##
rule all:
  input:
    expand("{structure}.{metric}.csv",
        structure=STRUCTURES,
        metric=METRICS)


wildcard_constraints:
    jobname="[\w-]+" # equivalent to {sample,\w+} - limits to alphabet letters

## Rules
## =====
## 

localrules: 
  calculate_beta_factor_profile, 
  install_sdf,
  calculate_clashes,
  calculate_ramachandran, 
  install_neural_plexer_apptainer, 
  download_neuralplexer_checkpoint_data, calculate_accessability_profile,
  help, clean, build_overview


## install_sdf:
##    Install the scidata flow tool using cargo.
## 
rule install_sdf:
  output:
    touch("results/checkpoint/sdf_install")
  shell:
    """
    cargo install scidataflow
    """




## install_neural_plexer_apptainer:
##    Installs the NeuralPlexer tool from the docker image as an Apptainer.
##    It uses the docker image as listed on 
##    [drailab docker hub](https://hub.docker.com/r/drailab/neuralplexer).
##
rule install_neural_plexer_apptainer:
  """Build the NeuralPlexer container

  Documentation:
    See [docs](https://apptainer.org/docs/user/latest/build_a_container.html)
    for more information.
  """
  output: "results/dependencies/neuralplexer/neuralplexer.sif"
  shell:
    """
    # the Docker command that would've done this is
    # docker pull drailab/neuralplexer
    apptainer build {output} docker://drailab/neuralplexer
    """

## download_neuralplexer_checkpoint_data:
##    Will download 8.3 Gb of NeuralPlexer model data from Zenodo, containing 
##    the model weights and testing data.
##
rule download_neuralplexer_checkpoint_data:
  output: "results/downloads/NeuralPlexerModelData.zip"
  shell:
    """
    wget -O {output} https://zenodo.org/records/10373581/files/neuralplexermodels_downstream_datasets_predictions.zip?download=1
    """

    
## Unzip the downloaded neuralplexer data:
##    The dataset contains the weights and the test data. In total 24 GB. Of
##    which the weights (data/neuralplexermodels_downstream_datasets_predictions
##    /models/complex_structure_prediction.ckp) make up 2.8 Gb.
##
rule unzip_neuralplexer_checkpoint_data:
  input: "results/downloads/NeuralPlexerModelData.zip"
  output: directory("results/dependencies/neuralplexer/data")
  shell:
    """
    mkdir {output}
    unzip {input} -d {output}
    """

def get_neuralplexer_output_pdb(wildcards):
  checkpoint_output = checkpoints.run_neural_plexer.get(**wildcards).output[0]
  print(f"Checking {checkpoint_output}")
  dirs = [f for f in os.listdir(checkpoint_output) if not f.startswith('.')]
  out = [f"{checkpoint_output}/{d}/prot_all.pdb" for d in dirs]
  return out


## run_neural_plexer:
##    Run the neuralplexer tool on a job directory, with a pdb file as the 
##    receptor and a SIF file as the ligand. Multiple SIF files are supported.
##
checkpoint run_neural_plexer:
  """neuralplexer rule

  note -- for now only sdf files are supported.
  """
  input:
    image="results/dependencies/neuralplexer/neuralplexer.sif",
    checkpoint="results/dependencies/neuralplexer/data/neuralplexermodels_downstream_datasets_predictions/models/complex_structure_prediction.ckpt",
    jobs="data/{jobname}.json"
  output:
    directory("results/data/{jobname}/")
  resources:
    mem_mb=32000,
    slurm_extra= "'--gres=gpu:a100:1'",
    constraint="gpu",
    tasks="1",
    cpus_per_task=10,
    mem="128G",
    slurm_partition='gpu1',
    runtime='1440',
    slurm_account="mpmp_gpu",
  shell:
    """
    python workflow/scripts/neuralplexer.py {input.image} \
      {input.checkpoint} {input.jobs} {output}
    """


## help:
##    Show the help.
##
rule help:
  input: "workflow/Snakefile"
  shell:
      "sed -n 's/^##//p' {input}"


## clean:                     
##    Clean all outputs from the results/data folder. Ignores the downloaded
##    NeuralPlexer dependencies.  
##
rule clean:
  shell:
      "rm -rf results/data"


## build_overview:            
##    Print the directed acyclic graph.
##
rule build_overview:
  output:
    "results/method.{fileformat}"
  shell:
    """
    snakemake -c 1 --forceall --dag | dot -T{wildcards.fileformat} > {output}
    """



## extract_plddt:
##    Extract the plddt of the model.
## 
rule calculate_beta_factor_profile:
  conda:
    "envs/mol.yml"
  input:
    get_neuralplexer_output_pdb
  output:
    "results/analysis/{jobname}/{name}.plddt.png",
    "results/analysis/{jobname}/{name}.plddt.csv",
  script:
    "scripts/mol/beta-factor.py"


## calculate_clashes:
##    Determine the number of steric clashes.
## 
rule calculate_clashes:
  conda:
    "envs/mol.yml"
  input:
    get_neuralplexer_output_pdb
  output:
    "results/analysis/{jobname}/{name}.clashes.csv",
  script:
    "scripts/mol/steric_clashes.py"


## calculate_ramachandran:
##    Perform a ramachandran analysis.
## 
rule calculate_ramachandran:
  conda:
    "envs/mol.yml"
  input:
    get_neuralplexer_output_pdb
  output:
    "results/analysis/{jobname}/{name}.ramachandran.png",
    "results/analysis/{jobname}/{name}.ramachandran.csv",
  script:
    "scripts/mol/ramachandran.py"


## calculate_accesability_profile:
##    Calculate the accessability profile of the calculated structures.
## 
rule calculate_accessability_profile:
  conda:
    "envs/mol.yml"
  input:
    get_neuralplexer_output_pdb
  output:
    "results/analysis/{jobname}/{name}.sasa.png",
    "results/analysis/{jobname}/{name}.sasa.csv",
  script:
    "scripts/mol/surface.py"
